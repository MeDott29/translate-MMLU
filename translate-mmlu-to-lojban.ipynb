{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\nfrom huggingface_hub import login\nlogin(token=f\"{secret_value_0}\",write_permission=True)  # Enter your HF token when prompted","metadata":{"execution":{"iopub.status.busy":"2024-10-25T03:40:33.066929Z","iopub.execute_input":"2024-10-25T03:40:33.067968Z","iopub.status.idle":"2024-10-25T03:40:34.022746Z","shell.execute_reply.started":"2024-10-25T03:40:33.067912Z","shell.execute_reply":"2024-10-25T03:40:34.021682Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom transformers import MarianMTModel, MarianTokenizer\nfrom tqdm.auto import tqdm\nimport torch\n\ndef process_dataset_in_batches(ds, model, tokenizer, batch_size=32):\n    \"\"\"Process the entire dataset in batches and create a new dataset with translations\"\"\"\n    device = next(model.parameters()).device\n    test_data = ds['test']\n    \n    # Initialize lists to store all data\n    all_questions = []\n    all_translations = []\n    all_subject = []\n    all_answers = []\n    all_choices = []\n    \n    # Process in batches\n    for i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing batches\"):\n        # Get batch of questions\n        batch_indices = range(i, min(i + batch_size, len(test_data)))\n        batch_questions = [test_data[j]['question'] for j in batch_indices]\n        \n        # Translate batch\n        encoded = tokenizer(batch_questions, return_tensors=\"pt\", padding=True, truncation=True)\n        encoded = {k: v.to(device) for k, v in encoded.items()}\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **encoded,\n                max_length=200,  # Increased for longer questions\n                num_beams=5,\n                length_penalty=1.0,\n                early_stopping=True,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n        \n        outputs = outputs.cpu()\n        batch_translations = [tokenizer.decode(output, skip_special_tokens=True) \n                            for output in outputs]\n        \n        # Store all data\n        all_questions.extend(batch_questions)\n        all_translations.extend(batch_translations)\n        all_subject.extend([test_data[j]['subject'] for j in batch_indices])\n        all_answers.extend([test_data[j]['answer'] for j in batch_indices])\n        all_choices.extend([{\n            'choice_A': test_data[j]['choices'][0],\n            'choice_B': test_data[j]['choices'][1],\n            'choice_C': test_data[j]['choices'][2],\n            'choice_D': test_data[j]['choices'][3]\n        } for j in batch_indices])\n        \n        # Optional: Save checkpoint every 1000 examples\n        if i % 1000 == 0 and i > 0:\n            save_checkpoint(i, all_questions, all_translations, all_subject, \n                          all_answers, all_choices)\n    \n    # Create dictionary for dataset\n    dataset_dict = {\n        'question': all_questions,\n        'translation': all_translations,\n        'subject': all_subject,\n        'answer': all_answers,\n        'choice_A': [choices['choice_A'] for choices in all_choices],\n        'choice_B': [choices['choice_B'] for choices in all_choices],\n        'choice_C': [choices['choice_C'] for choices in all_choices],\n        'choice_D': [choices['choice_D'] for choices in all_choices]\n    }\n    \n    # Create and save dataset\n    translated_dataset = Dataset.from_dict(dataset_dict)\n    translated_dataset.push_to_hub(\n        \"tinycrops/mmlu-lojban\",\n        private=False  # Set to False if you want it public\n    )\n    \n    return translated_dataset\n\ndef save_checkpoint(index, questions, translations, subjects, answers, choices):\n    \"\"\"Save checkpoint to disk\"\"\"\n    checkpoint = {\n        'index': index,\n        'questions': questions,\n        'translations': translations,\n        'subjects': subjects,\n        'answers': answers,\n        'choices': choices\n    }\n    torch.save(checkpoint, f'translation_checkpoint_{index}.pt')\n\n# Main execution\nprint(\"Loading dataset and model...\")\nds = load_dataset(\"cais/mmlu\", \"all\")\n\nmodel_name = \"woctordho/lojban-translation\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\n# Move model to GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\nprint(f\"Using device: {device}\")\n\nprint(\"\\nStarting translation process...\")\ntranslated_dataset = process_dataset_in_batches(ds, model, tokenizer)\n\nprint(\"\\nTranslation complete! Dataset uploaded to Hugging Face Hub.\")\nprint(\"Dataset stats:\", translated_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T03:40:34.024800Z","iopub.execute_input":"2024-10-25T03:40:34.025201Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loading dataset and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"babb365d09f8489cb39ea2bd57994c7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c02df696a74bb19112197bcfd54f23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/3.50M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bb9a1c247d34db9b4314a4c9f805ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/408k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab8daec452914cdcbbdf1a490b137fa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev-00000-of-00001.parquet:   0%|          | 0.00/76.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d3d5df7313440ca0f3ab652ac55388"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"auxiliary_train-00000-of-00001.parquet:   0%|          | 0.00/47.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fccade4824d942a4a82117aff33fbc5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/14042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601d7c9531e84f65b5efd8be0751bd91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1531 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f605dd7cb5499fb697a7df1319a4f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/285 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e94549c560414a2eacffb016a12c9762"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating auxiliary_train split:   0%|          | 0/99842 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa4edba2d7d64f129d5d6035cce2515b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa77d4ea16545569815ac865e140128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/806k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4065155233ea4ea789c318745f914531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/805k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38a530e01724676ab430371394f3c51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.75M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00cb1810e086465fa5fe99aa40c44098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8411e39a5ef142debb569ffc1914800d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cadf753b69d14af4b6620873de262470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/310M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09244436d3e948139f6d3e77c7ab8501"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n\nStarting translation process...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing batches:   0%|          | 0/439 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b0a3b295e83465b8f9fdee33b0d4156"}},"metadata":{}}]}]}